{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Yao Group Page.</p>"},{"location":"fbf/","title":"Overview","text":"This project focuses on fixed boundary flows with canonical interpretability as principal components extended on non-linear Riemannian manifolds. Our primary objective is to identify a flow with fixed starting and ending points for noisy multivariate data sets lying near an embedded non-linear Riemannian manifold. In geometric terms, the fixed boundary flow represents an optimal curve that traverses the data cloud, maintaining two unchanging endpoints. At any given point along this flow, we seek to maximize the inner product between the locally calculated vector field and the tangent vector of the flow. The formal definition arises from an optimization problem utilizing the intrinsic metric of the manifolds.  <p>An implementation in R is available on Github:   Detailed description and discussion can be found in paper:   To cite:  <pre><code>@article{yao2023fixed,\n  title={Random Fixed Boundary Flows},\n  author={Yao, Zhigang and Xia, Yuqing and Fan, Zengyan},\n  journal={arXiv preprint arXiv:1904.11332},\n  year={2023}\n}\n</code></pre></p>"},{"location":"fbf/pages/talks/","title":"Selected Talks","text":""},{"location":"fbf/pages/talks/#first-symposium-of-geometry-and-statistics-in-china","title":"First Symposium of Geometry and Statistics in China","text":"<p> Yanqi Lake Beijing Institute of Mathematical Sciences and Applications, \\(\\quad\\) Yau Center at Tsinghua University  July 29, 2023</p>"},{"location":"mf/","title":"Overview","text":"In recent years, there has been a growing interest in non-Euclidean statistical analysis, particularly in the pursuit of recovering a low-dimensional structure, referred to as a manifold, that underlies high-dimensional data. The successful recovery of this manifold is contingent on certain noise concentration conditions. Previous methods tackle this challenge by approximating the manifold based on tangent space estimations at each data sample. While these methods offer theoretical convergence guarantees, they assume either noise-free data or noise with bounded characteristics. In practical scenarios where unbounded noise is common, the tangent space estimations at noisy samples become inherently imprecise, potentially introducing inaccuracies when fitting the manifold.  In response to this challenge, our research project introduces a novel approach. Instead of estimating tangent spaces at the original data samples, we directly estimate these spaces at projected points on the underlying manifold. This strategic shift aims to mitigate errors caused by unbounded noise, resulting in more accurate manifold fitting.  Our research, encompassing our 2019 paper (yx) and subsequent work in 2023 (ysl), centers on non-Euclidean statistical analysis, specifically the recovery of low-dimensional manifolds from high-dimensional data. Unlike existing methods relying on tangent space estimations at data samples, we directly estimate these spaces at manifold-projected points, enhancing accuracy and addressing unbounded noise. Our initial paper (yx) introduced a practical algorithm for manifold fitting, and our 2023 work (ysl) refines the algorithm and establishes superior error bounds. These contributions significantly advance non-Euclidean statistical analysis.  <p>Implementations of manifold algorithms in Matlab and Python are available on Github:   Detailed description and discussion can be found in papers:  yx ysl To cite: </p> <pre><code>@article{yao2019manifold,\n  title={Manifold fitting under unbounded noise},\n  author={Yao, Zhigang and Xia, Yuqing},\n  journal={arXiv preprint arXiv:1909.10228},\n  year={2019}\n}\n</code></pre> <pre><code>@article{yao2023manifold,\n  title={Manifold Fitting},\n  author={Yao, Zhigang and Su, Jiaji and Li, Bingjie and Yau, Shing-Tung},\n  journal={arXiv preprint arXiv:2304.07680},\n  year={2023}\n}\n</code></pre>"},{"location":"mf/pages/model_setting/","title":"Model Setting","text":"<p>Let \\(\\mathcal{M}\\) be a \\(d\\)-dimensional smooth latent manifold embedded in the ambient space \\(\\mathbb{R}^D\\). In this problem, we focus on a random vector \\(Y \\in \\mathbb{R}^D\\) that can be expressed as</p> \\[     Y = X + \\xi, \\] <p>where \\(X \\in \\mathbb{R}^D\\) is an unobserved random vector following a distribution \\(\\omega\\) supported on the latent manifold \\(\\mathcal{M}\\), and \\(\\xi \\sim \\phi_\\sigma\\) represents the ambient-space observation noise, independent of \\(X\\), with a standard deviation \\(\\sigma\\). The distribution of \\(Y\\) can be viewed as the convolution of \\(\\omega\\) and \\(\\phi_\\sigma\\), whose density at point \\(y\\) can be expressed as</p> \\[     \\nu(y) = \\int_\\mathcal{M} \\phi_\\sigma(y-x)\\omega(x)d x. \\] <p>Assume \\(\\mathcal{Y} = \\{y_i\\}_{i=1}^N \\subset \\mathbb{R}^D\\) is the collection of observed data points, also in the form of</p> \\[     y_i = x_i + \\xi_i, \\quad \\text{ for } i = 1,\\cdots,N,  \\] <p>with \\((y_i, x_i,\\xi_i)\\) being \\(N\\) independent and identical realizations of \\((Y,X,\\xi)\\). Based on \\(\\mathcal{Y}\\), we construct an estimator \\(\\widehat{\\mathcal{M}}\\) for \\(\\mathcal{M}\\) and provide theoretical justification for it under the following main assumptions:</p> <ul> <li> <p>The latent manifold \\(\\mathcal{M}\\) is a compact and twice-differentiable \\(d\\)-dimensional sub-manifold, embedded in the ambient space \\(\\mathbb{R}^D\\). Its volume with respect to the \\(d\\)-dimensional Hausdorff measure is upper bounded by \\(V\\), and its reach1 is lower bounded by a fixed constant \\(\\tau\\).</p> </li> <li> <p>The distribution \\(\\omega\\) is a uniform distribution, with respect to the \\(d\\)-dimensional Hausdorff measure, on \\(\\mathcal{M}\\).</p> </li> <li> <p>The noise distribution \\(\\phi_\\sigma\\) is a Gaussian distribution supported on \\(\\mathbb{R}^D\\) with density function </p> \\[   \\phi_\\sigma (\\xi)= (\\frac{1}{2\\pi \\sigma^2})^{\\frac{D}{2}}\\exp{(-\\frac{\\|\\xi\\|_2^2}{2\\sigma^2})}. \\] </li> <li> <p>The intrinsic dimension \\(d\\) and noise standard deviation \\(\\sigma\\) are known.</p> </li> </ul> <ol> <li> <p>A non-negative quantity measuring the curvature of a manifold.\u00a0\u21a9</p> </li> </ol>"},{"location":"mf/pages/talks/","title":"Selected Talks","text":""},{"location":"mf/pages/talks/#harvard-probability-seminar","title":"Harvard Probability Seminar","text":"<p> Center of Mathematical Sciences and Applications, Harvard University    February 15, 2023</p>"},{"location":"mf/pages/talks/#harvard-conference-on-geometry-and-statistics","title":"Harvard Conference on Geometry and Statistics","text":"<p> Center of Mathematical Sciences and Applications, Harvard University   February 27, 2023</p>"},{"location":"mf/pages/talks/#tsinghua-seminar","title":"Tsinghua Seminar","text":"<p> Yau Mathematical Sciences Center, Tsinghua University  July 4, 2023</p>"},{"location":"mf/pages/talks/#pujiang-innovation-forum-of-advances-of-basic-science","title":"Pujiang Innovation Forum of Advances of Basic Science","text":"<p> Shanghai  July 24, 2023</p>"},{"location":"mf/pages/talks/#royal-statistical-society-seminar","title":"Royal Statistical Society Seminar","text":"<p> University of Kent, Canterbury  November 23, 2023</p>"},{"location":"mf/pages/talks/#cambridge-statistics-series-talk","title":"Cambridge Statistics Series Talk","text":"<p> Centre for Mathematical Sciences, Cambridge  November 24, 2023</p>"},{"location":"mf/pages/test2/","title":"Test page 2","text":"<p>This website is still under construction, please come back later.</p>"},{"location":"mf/pages/test2/#l2-title","title":"L2 title","text":"<p>L2 title will be shown in the table of contents.</p>"},{"location":"mfcgan/","title":"Overview","text":"This project pioneers a novel approach using neural networks within the generative adversarial network (GAN) framework for manifold fitting, a crucial challenge in non-linear data analysis. This method learns mappings between low-dimensional latent spaces and high-dimensional ambient spaces, akin to Riemannian exponential and logarithmic maps, providing manifold estimations, data projection, and even data generation within the manifold.  Through extensive simulations and real-data experiments, we demonstrate the precision and computational efficiency of our approach in capturing the underlying manifold's structure. This advancement holds significant potential in fields like statistics and computer science, offering control over manifold dimensionality and smoothness while enhancing data analysis. By integrating powerful neural network architectures with generative adversarial techniques, our research unlocks new possibilities for manifold fitting, spanning applications from dimensionality reduction and data visualization to authentic data generation, paving the way for future advancements in non-linear data analysis and inspiring further scholarly exploration.  <p>An implementation in Pytorch is available on Github:   Detailed description and discussion can be found in paper:   To cite:  <pre><code>@article{doi:10.1073/pnas.2311436121,\nauthor = {Zhigang Yao  and Jiaji Su  and Shing-Tung Yau },\ntitle = {Manifold fitting with CycleGAN},\njournal = {Proceedings of the National Academy of Sciences},\nvolume = {121},\nnumber = {5},\npages = {e2311436121},\nyear = {2024},\ndoi = {10.1073/pnas.2311436121},\nURL = {https://www.pnas.org/doi/abs/10.1073/pnas.2311436121}\n}\n</code></pre></p>"},{"location":"pb/","title":"Overview","text":"This project delves into the classification problem, with a specific focus on non-linear methods applied to datasets residing on embedded non-linear Riemannian manifolds within higher-dimensional ambient spaces. Our objective is to establish a classification boundary for labeled classes, leveraging the intrinsic metric of these manifolds.  In pursuit of an optimal boundary that effectively separates the classes, we introduce a novel concept - the \"principal boundary.\" In the context of classification, the principal boundary is defined as an optimal curve positioned between the principal flows originating from two distinct classes of data. At every point along this boundary, it maximizes the margin between the two classes. We estimate the quality and direction of this boundary, guided by the two principal flows. We demonstrate that the principal boundary aligns locally with the decision boundary derived from a support vector machine, ensuring consistency in classification outcomes.   We also present optimality and convergence properties of both the random principal boundary and its population counterpart. To provide practical insights, we illustrate how to discover, apply, and interpret the principal boundary through an application to real-world data. Additional supplementary materials for this article are accessible online.  <p>Detailed description and discussion can be found in paper:   To cite: </p> <pre><code>@article{yao2020principal,\n    author = {Yao, Zhigang and Zhang, Zhenyue},\n    title = {Principal Boundary on Riemannian Manifolds},\n    journal = {Journal of the American Statistical Association},\n    volume = {115},\n    number = {531},\n    pages = {1435-1448},\n    year  = {2020}\n}\n</code></pre>"},{"location":"pb/pages/talks/","title":"Selected Talks","text":""},{"location":"pb/pages/talks/#duke-math-seminar","title":"Duke Math Seminar","text":"<p> Department of Mathematics, Duke   March 21, 2023</p>"},{"location":"pf/","title":"Overview","text":"This project delves into the extension of Principal Component Analysis (PCA) to multivariate datasets constrained by nonlinear relationships, residing on Riemannian manifolds. Our primary objective is to identify curves on these manifolds that preserve their inherent interpretability as principal components while being flexible enough to encompass non-geodesic variations.  We introduce the concept of a \"principal flow,\" which represents a curve on the manifold passing through the data mean. This curve possesses the unique property that, at any given point, the tangent velocity vector aligns with the local first eigenvector derived from tangent space PCA, subject to smoothness constraints. Essentially, a particle following the principal flow path seeks to traverse the most variable data path, taking into account smoothness limitations. The rigorous definition of a principal flow is formulated as a Lagrangian variational problem, subsequently reduced to an Ordinary Differential Equation (ODE) problem through the Euler\u2013Lagrange method. We provide conditions for existence and uniqueness and outline an algorithm for numerical problem-solving. We also introduce higher-order principal flows.  Furthermore, we demonstrate that global principal flows yield traditional principal components in an Euclidean space. Through illustrative examples, we showcase the principal flow's ability to capture patterns of variation that may elude other manifold PCA methods.  <p>Detailed description and discussion can be found in paper:   To cite: </p> <pre><code>@article{panaretos2014principal,\n    title={Principal flows},\n    author={Panaretos, Victor M and Pham, Tung and Yao, Zhigang},\n    journal={Journal of the American Statistical Association},\n    volume={109},\n    number={505},\n    pages={424--436},\n    year={2014}\n}\n</code></pre>"},{"location":"pf/pages/talks/","title":"Selected Talks","text":""},{"location":"pf/pages/talks/#carnegie-mellon-seminar","title":"Carnegie Mellon Seminar","text":"<p> Department of Statistics, Carnegie Mellon University  October 3, 2022</p>"},{"location":"pf/pages/talks/#columbia-statistics-talk","title":"Columbia Statistics Talk","text":"<p> Department of Statistics, Columbia University  October 24, 2022</p>"},{"location":"pf/pages/talks/#member-seminar","title":"Member Seminar","text":"<p> Center of Mathematical Sciences and Applications, Harvard University   July 10, 2022</p>"},{"location":"pf/pages/test/","title":"Test page","text":""},{"location":"scamf/","title":"Overview","text":"This project ...  <p>An implementation in Pytorch is available on Github:   Detailed description and discussion can be found in paper:   To cite:  <pre><code>@article{yao2024scamf,\n  title={Single-Cell Analysis via Manifold Fitting},\n  author={Yao, Zhigang and Li, Bingjie and Lu, Yukun and Yau, Shing-Tung},\n  journal = {Scientific Report},\n  year={2024}\n}\n</code></pre></p>"},{"location":"sub/","title":"Overview","text":"This project introduces a novel approach to uncovering principal components within multivariate datasets residing on embedded nonlinear Riemannian manifolds within higher-dimensional spaces. Our goal is to enhance the geometric interpretation of Principal Component Analysis (PCA) while retaining the capacity to capture non-geodesic modes of variation in the data.  We introduce the concept of a \"principal sub-manifold,\" which is essentially a manifold passing through a reference point. At any given point along this sub-manifold, it extends in the direction characterized by the highest variation within the space spanned by the eigenvectors derived from local tangent space PCA. This concept builds upon previous work on principal flows, specifically in cases where the sub-manifold has a dimension greater than one. In essence, the principal sub-manifold offers a versatile extension of the principal flow, capable of capturing higher-dimensional variations within the data.  We demonstrate that the principal sub-manifold encompasses the space spanned by the traditional principal components in Euclidean space. Through illustrative examples, we elucidate how to discover, utilize, and interpret a principal sub-manifold. Additionally, we showcase an application in shape analysis to underscore its practical relevance and utility.  <p>An implementation in Matlab is available on Github:   Detailed description and discussion can be found in paper:   To cite: </p> <p><pre><code>@article{yao2016principal,\n  title={Principal sub-manifolds},\n  author={Yao, Zhigang and Eltzner, Benjamin and Pham, Tung},\n  journal={arXiv preprint arXiv:1604.04318},\n  year={2016}\n}\n</code></pre> <pre><code>@article{yao2023hunting,\n  title={Hunting Principal Submanifolds: New Theories and Methods},\n  author={Yao, Zhigang and Li, Bingjie and Tran, Van Do and Zhang, Zhenyue},\n  journal={Technical Report},\n  year={2023}\n}\n</code></pre></p>"},{"location":"sub/pages/talks/","title":"Selected Talks","text":""},{"location":"sub/pages/talks/#stanford-statistics-talk","title":"Stanford Statistics Talk","text":"<p> Department of Statistics, Stanford    February 7, 2023</p>"},{"location":"sub/pages/talks/#tsinghua-seminar","title":"Tsinghua Seminar","text":"<p> Yau Mathematical Sciences Center, Tsinghua University  June 13, 2023</p>"}]}